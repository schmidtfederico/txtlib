% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizers.R
\name{UAX29Tokenizer}
\alias{UAX29Tokenizer}
\title{Tokenize texts using Unicode Standard Annex #29 text segmentation rules.}
\usage{
UAX29Tokenizer(
  ignored_terms = as.character(c()),
  case_sensitive_aliases = as.character(c()),
  case_insensitive_aliases = as.character(c()),
  casing_transformation = NA,
  ngrams_size = 1L,
  min_term_length = 1L,
  stemming_language = NA,
  word_token_categories = NULL,
  non_word_token_categories = NULL,
  locale = NULL
)
}
\arguments{
\item{ignored_terms}{A character array of terms to ignore when processing texts.}

\item{case_sensitive_aliases}{A named character array that maps case sensitive terms to their replacements (e.g. c("US" = "U.S.")).}

\item{case_insensitive_aliases}{A named character array that maps case insensitive terms to their replacements (e.g. c("it's" = "it_is")).}

\item{casing_transformation}{Casing transformation function name: "lower" or NA.}

\item{ngrams_size}{Maximum size of generated ngrams.}

\item{min_term_length}{Minimum length a term should have to be considered a token.}

\item{stemming_language}{Language used when stemming terms. If NA, no stemming is done.}

\item{word_token_categories}{Only terms with at least one character belonging to these Unicode General Categories will be considered a token (defaults to all).}

\item{non_word_token_categories}{Terms with any of these General Categories will be skipped (empty list by default).}

\item{locale}{Unicode locale to use when parsing texts.}
}
\description{
Tokenize texts using Unicode Standard Annex #29 text segmentation rules.
}
\seealso{
https://unicode.org/reports/tr29/
}
